{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3da81f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow/Keras: 2.9.0\n",
      "pandas: 1.4.4\n",
      "numpy: 1.22.3\n",
      "sklearn: 0.23.2\n",
      "plotly: 5.11.0\n"
     ]
    }
   ],
   "source": [
    "## IMPORT LIBRARIES\n",
    "# General libraries\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "from natsort import natsorted\n",
    "import sys\n",
    "import re\n",
    "from matplotlib import gridspec\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# Tensorflow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras # for building Neural Networks\n",
    "print('Tensorflow/Keras: %s' % keras.__version__) # print version\n",
    "from tensorflow.keras.models import Sequential # for creating a linear stack of layers for our Neural Network\n",
    "import tensorflow.python.keras.metrics \n",
    "from tensorflow.keras import Input # for instantiating a keras tensor\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout # for creating regular densely-connected NN layers.\n",
    "from tensorflow.keras.callbacks import History\n",
    "import keras_tuner as kt \n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd # for data manipulation\n",
    "print('pandas: %s' % pd.__version__) # print version\n",
    "import numpy as np # for data manipulation\n",
    "print('numpy: %s' % np.__version__) # print version\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Sklearn\n",
    "import sklearn # for model evaluation\n",
    "print('sklearn: %s' % sklearn.__version__) # print version\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix,plot_confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif , chi2\n",
    "\n",
    "# Visualization\n",
    "import plotly \n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "print('plotly: %s' % plotly.__version__) # print version\n",
    "#from pyts.image import RecurrencePlot\n",
    "#from PIL import Image\n",
    "#from matplotlib import cm\n",
    "#from ripser import Rips\n",
    "#from persim import PersImage\n",
    "#from persim import PersistenceImager\n",
    "%matplotlib inline\n",
    "\n",
    "# Feature importance\n",
    "import shap\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "    le = LabelEncoder();\n",
    "    le.fit(y_train)\n",
    "    y_train_enc = le.transform(y_train)\n",
    "    y_test_enc = le.transform(y_test)\n",
    "    return y_train_enc, y_test_enc\n",
    "\n",
    "##seed for reproducibility\n",
    "seed_value = 1\n",
    "# Set the `python` built-in pseudo-random generator at a fixed value\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Set the `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b3d1cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev: [1, 2, 3, 4, 5, 6, 7], classes : ['Control_100mV', 'CTPR16_100mV']\n",
      "24090\n",
      "6\n",
      "X TRAIN (19272, 777)\n",
      "X TEST (4818, 777)\n",
      "Y TRAIN (19272,)\n",
      "Y TEST (4818,)\n"
     ]
    }
   ],
   "source": [
    "dev_num_all = [1,2,3,4,5,6,7]\n",
    "conditions = ['Control_100mV','CTPR16_100mV']\n",
    "#dev_num_test =  [6,7,8]\n",
    "\n",
    "##### Select data for modeling\n",
    "\n",
    "print(f'dev: {dev_num_all}, classes : {conditions}')\n",
    "full_data = pd.concat(map(pd.read_csv,[f'./dev_{dev_num_all[0]}_{conditions[0]}_all_extracted_features.csv',\n",
    "                                       f'./dev_{dev_num_all[0]}_{conditions[1]}_all_extracted_features.csv',\n",
    "                                       f'./dev_{dev_num_all[1]}_{conditions[0]}_all_extracted_features.csv',\n",
    "                                       f'./dev_{dev_num_all[1]}_{conditions[1]}_all_extracted_features.csv',\n",
    "                                       f'./dev_{dev_num_all[2]}_{conditions[0]}_all_extracted_features.csv',\n",
    "                                       f'./dev_{dev_num_all[2]}_{conditions[1]}_all_extracted_features.csv',\n",
    "                                       f'./dev_{dev_num_all[3]}_{conditions[0]}_all_extracted_features.csv',\n",
    "                                       f'./dev_{dev_num_all[3]}_{conditions[1]}_all_extracted_features.csv',\n",
    "                                       f'./dev_{dev_num_all[4]}_{conditions[0]}_all_extracted_features.csv',\n",
    "                                       f'./dev_{dev_num_all[4]}_{conditions[1]}_all_extracted_features.csv',\n",
    "                                       f'./dev_{dev_num_all[5]}_{conditions[0]}_all_extracted_features.csv',\n",
    "                                       f'./dev_{dev_num_all[5]}_{conditions[1]}_all_extracted_features.csv',\n",
    "                                       f'./dev_{dev_num_all[6]}_{conditions[0]}_all_extracted_features.csv',\n",
    "                                       f'./dev_{dev_num_all[6]}_{conditions[1]}_all_extracted_features.csv',\n",
    "                                       #f'./dev_{dev_num_all[7]}_{conditions[0]}_all_extracted_features.csv',\n",
    "                                       #f'./dev_{dev_num_all[7]}_{conditions[1]}_all_extracted_features.csv'\n",
    "                                       #f'./dev_{dev_num_train[8]}_{conditions[0]}_all_extracted_features.csv',\n",
    "                                       #f'./dev_{dev_num_train[8]}_{conditions[1]}_all_extracted_features.csv'\n",
    "                                       ]),ignore_index=True).sample(frac=1,random_state=seed_value)\n",
    "\n",
    "print(len(full_data))\n",
    "\n",
    "nan_cols = [i for i in full_data.columns if full_data[i].isnull().any()]\n",
    "\n",
    "print(len(nan_cols))\n",
    "\n",
    "##### Create training and testing samples\n",
    "full_data_wo_nan = full_data.drop(nan_cols,axis=1)\n",
    "\n",
    "X = full_data_wo_nan.drop(['label','dev_label_id'],axis=1).values\n",
    "\n",
    "y = full_data_wo_nan['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "\n",
    "print('X TRAIN',X_train.shape)\n",
    "print('X TEST',X_test.shape)\n",
    "\n",
    "print('Y TRAIN',y_train.shape)\n",
    "print('Y TEST',y_test.shape)\n",
    "\n",
    "##### Data Scaling\n",
    "\n",
    "scaler = MinMaxScaler()  #StandardScaler, MinMaxScaler\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled) #, columns = X_train.columns.tolist())\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled) #, columns = X_train.columns.tolist())\n",
    "#print(X_train_scaled_df.head())\n",
    "\n",
    "# prepare target labels as 0 or 1\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "num_top_feat = 50\n",
    "\n",
    "# Define feature selection\n",
    "fs = SelectKBest(score_func=chi2, k=num_top_feat) \n",
    "### apply feature selection on scaled data\n",
    "X_train_selected = fs.fit_transform(X_train_scaled, y_train_enc)\n",
    "#print('X_train_selected shape :', X_train_selected.shape)\n",
    "##X_test_selected = fs.transform(X_test_scaled)\n",
    "##print(X_test_selected.shape)\n",
    "\n",
    "# Get columns to keep and create new dataframe with those only\n",
    "cols = fs.get_support(indices=True)\n",
    "X_train_selected_df = X_train_scaled_df.iloc[:,cols]\n",
    "X_test_selected_df = X_test_scaled_df.iloc[:,cols]\n",
    "#print(X_train_selected_df.head())\n",
    "    \n",
    "#pca = PCA(n_components=10)\n",
    "#train_pca = pca.fit_transform(X_train_scaled)\n",
    "#test_pca = pca.transform(X_test_scaled)\n",
    "#X_train_pca = pd.DataFrame(data = train_pca, columns = ['pc1','pc2','pc3','pc4','pc5','pc6','pc7','pc8','pc9','pc10'])\n",
    "#X_test_pca = pd.DataFrame(data = test_pca, columns = ['pc1','pc2','pc3','pc4','pc5','pc6','pc7','pc8','pc9','pc10'])\n",
    "#\n",
    "#print('X TRAIN PCA',X_train_pca.info)\n",
    "#print('X TEST PCA',X_test_pca.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518b0e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def lr_step_decay(epoch, lr):\n",
    "#    drop_rate = 0.1\n",
    "#    epochs_drop = 20.0\n",
    "#    return initial_learning_rate * math.pow(drop_rate, math.floor(epoch/epochs_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10c6def1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 02m 19s]\n",
      "val_Accuracy: 0.9613488912582397\n",
      "\n",
      "Best val_Accuracy So Far: 0.9693903923034668\n",
      "Total elapsed time: 00h 06m 38s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "\n",
      "The hyperparameter search is complete. The optimal number of units in each densely-connected layer is \n",
      "1024, the optimal learning rate for the optimizer is 0.001, the \n",
      "best number of epochs is 50 and the best batch size is 256.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        # Tune the number of units in the first Dense layer\n",
    "        # Choose an optimal value between 256-1024\n",
    "        hp_units = hp.Int('units', min_value=256, max_value=1024, step=32)\n",
    "        model.add(Input(shape=(50,),name=\"Input-layer\")) #777\n",
    "        model.add(Dense(units=hp_units, activation='relu',name=\"layer1\"))\n",
    "        model.add(Dense(units=hp_units, activation='relu',name=\"layer2\"))\n",
    "        model.add(Dense(units=hp_units, activation='relu',name=\"layer3\"))\n",
    "        model.add(Dense(1, activation='sigmoid', name='Output-Layer'))\n",
    "      \n",
    "        # Tune the learning rate for the optimizer\n",
    "        # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "        hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 5e-3,1e-3])\n",
    "      \n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['Accuracy', 'Precision', 'Recall'])\n",
    "        return model\n",
    "    # Tune the epoch and batch size for the fit\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size = hp.Int('batch_size', 32, 256, step=32),\n",
    "            epochs = hp.Int('epochs', 10, 50, step = 10),\n",
    "            **kwargs)\n",
    "\n",
    "tuner = kt.BayesianOptimization(MyHyperModel(), objective=\"val_Accuracy\", max_trials=5,seed=seed_value, overwrite=True, \n",
    "                                directory=os.path.normpath('C:/Users/goura/Desktop'), project_name=\"hp_tune_mm_1\")\n",
    "##use short path names to save the project in dir\n",
    "\n",
    "tuner.search(X_train_selected_df, y_train_enc,validation_split=0.2)\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in each densely-connected layer is \n",
    "{best_hps.get('units')}, the optimal learning rate for the optimizer is {best_hps.get('learning_rate')}, the \n",
    "best number of epochs is {best_hps.get('epochs')} and the best batch size is {best_hps.get('batch_size')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66a2a376",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "482/482 [==============================] - 11s 18ms/step - loss: 0.3884 - Accuracy: 0.8187 - precision: 0.8098 - recall: 0.8612 - val_loss: 0.3289 - val_Accuracy: 0.8625 - val_precision: 0.8182 - val_recall: 0.9491\n"
     ]
    }
   ],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train_selected_df,y_train_enc,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00d03ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch: 1\n"
     ]
    }
   ],
   "source": [
    "val_acc_per_epoch = history.history['val_Accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fcb4b05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "482/482 [==============================] - 9s 15ms/step - loss: 0.3859 - Accuracy: 0.8212 - precision: 0.8110 - recall: 0.8651 - val_loss: 0.3245 - val_Accuracy: 0.8703 - val_precision: 0.8299 - val_recall: 0.9472\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x170a72d43a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "# Retrain the model\n",
    "hypermodel.fit(X_train_selected_df, y_train_enc,epochs=best_epoch, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da6c8ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/151 [==============================] - 1s 6ms/step - loss: 0.3260 - Accuracy: 0.8686 - precision: 0.8319 - recall: 0.9485\n",
      "[test loss, test accuracy,test precision, test recall]: [0.3259658217430115, 0.8686177134513855, 0.8318732976913452, 0.9485209584236145]\n"
     ]
    }
   ],
   "source": [
    "eval_result = hypermodel.evaluate(X_test_selected_df, y_test_enc)\n",
    "print(\"[test loss, test accuracy,test precision, test recall]:\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ea2ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
